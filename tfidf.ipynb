{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFの考え方。\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting\n",
    "\n",
    "文書をたくさん集める。\n",
    "各文書と単語ごとにTF-IDFを計算する。\n",
    "\n",
    "ここでTFとはterm frequencyの略で、単語の頻度。\n",
    "ある文書ののべ全単語数に対し、その文書におけるある単語の出現頻度。\n",
    "\n",
    "IDFとはinverse document frequencyの略で、ある単語が含まれる文書の割合DFの逆数。\n",
    "いくつかの方法があるが、今回用いるのは\n",
    "\n",
    "$$\n",
    "idf(t) = \\log \\frac{n_d}{1+df(d,t)}\n",
    "$$\n",
    "$$\n",
    "idf(t) = \\log \\frac{n_d}{df(d,t)} + 1\n",
    "$$\n",
    "のいずれか。\n",
    "\n",
    "ここで$n_d$は文書の数、$df(d,t)$は単語$t$が含まれる文書の数。\n",
    "\n",
    "例えば与えられた文書が以下の三つだとしよう。\n",
    "\n",
    "- This is a pen.\n",
    "- This is an apple.\n",
    "- This is a pineapple and this is a pen.\n",
    "\n",
    "一番上の文書において、penのTFは1/4, IDFはlog(3/1)+1となり、\n",
    "tf-idf(1, pen)=1/4 * (log3 + 1)となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7049094889309326"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "x, y = 1/3 * (math.log(3.0/2.0) + 1), 1/3 * (math.log(3.0/3.0) + 1)\n",
    "x / math.sqrt(x**2+y**2+y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['This is a pen.',\n",
    "          'This is an apple.',\n",
    "          'This is a pineapple and this is a pen.',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 2, 1, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'and', 'apple', 'is', 'pen', 'pineapple', 'this']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.50154891,  0.70490949,\n",
       "         0.        ,  0.50154891],\n",
       "       [ 0.63834075,  0.        ,  0.63834075,  0.30417279,  0.        ,\n",
       "         0.        ,  0.30417279],\n",
       "       [ 0.        ,  0.48421906,  0.        ,  0.46146595,  0.32428715,\n",
       "         0.48421906,  0.46146595]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# smooth_idfはIDFに+1するか否か\n",
    "transformer = TfidfTransformer(smooth_idf = False)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                    token_pattern=r'\\b\\w+\\b')\n",
    "# ngram_rangeは特徴量として取り出す単語数の範囲\n",
    "# token_patternは単語として認識するものの正規表現、上だと1文字も単語として見なされる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [2, 1, 1, 0, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a pen',\n",
       " 'a pineapple',\n",
       " 'an',\n",
       " 'an apple',\n",
       " 'and',\n",
       " 'and this',\n",
       " 'apple',\n",
       " 'is',\n",
       " 'is a',\n",
       " 'is an',\n",
       " 'pen',\n",
       " 'pineapple',\n",
       " 'pineapple and',\n",
       " 'this',\n",
       " 'this is']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idfを用いてIMDBデータセットの分析を行う。\n",
    "\n",
    "ここではロジスティック回帰で分類するが、他の方法も試した上で交差検証を行いながら分類精度を向上させるための工夫を行ってみよう。\n",
    "ある程度精度が出るようになったらtestデータで試して見ること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train,  y_train), (x_test, y_test) = imdb.load_data()\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts=[]\n",
    "\n",
    "import collections\n",
    "for t in x_train:\n",
    "    counts.append(collections.Counter(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,  15., ...,   0.,   0.,   0.],\n",
       "       [  1.,   0.,  15., ...,   0.,   0.,   0.],\n",
       "       [  1.,   0.,   9., ...,   0.,   0.,   0.],\n",
       "       ..., \n",
       "       [  1.,   0.,  13., ...,   0.,   0.,   0.],\n",
       "       [  1.,   0.,   5., ...,   0.,   0.,   0.],\n",
       "       [  1.,   0.,  10., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer()\n",
    "x_train_counts = v.fit_transform(counts)\n",
    "x_train_counts.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 88585), (25000, 88585))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_test=[]\n",
    "\n",
    "for t in x_test:\n",
    "    counts_test.append(collections.Counter(t))\n",
    "\n",
    "x_test_counts = v.transform(counts_test)\n",
    "x_train_counts.shape, x_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 88585), (25000,), (25000, 88585), (25000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf = False)\n",
    "\n",
    "x_train_tfidf = transformer.fit_transform(x_train_counts).toarray()\n",
    "x_test_tfidf = transformer.transform(x_test_counts).toarray()\n",
    "\n",
    "x_train_tfidf.shape, y_train.shape, x_test_tfidf.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88551999999999997"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':[0.01, 1]}\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, parameters)\n",
    "clf.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.best_score_, clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_str = [' '.join([str(t) for t in d]) for d in x_train]\n",
    "x_test_str = [' '.join([str(t) for t in d]) for d in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(2, 2),\n",
    "                        token_pattern=r'\\b\\w+\\b')\n",
    "x_train_bigram_tfidf = tfidf.fit_transform(x_train_str).toarray()\n",
    "x_test_bigram_tfidf = tfidf.transform(x_test_str).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_bigram_tfidf, y_train)\n",
    "clf.score(x_test_bigram_tfidf, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
